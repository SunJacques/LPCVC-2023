{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet18 training\n",
    "\n",
    "created on 20/06/2023"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import glob\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LPCVCDataset(Dataset):\n",
    "    def __init__(self, datapath, transform, n_class=14, train=True, patch=False):\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.transform = transform\n",
    "        self.n_class = n_class\n",
    "        self.train = train\n",
    "\n",
    "        self.patches = patch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            files = glob.glob(os.path.join(self.datapath + 'train/IMG/train', \"*.png\"))\n",
    "        else:\n",
    "            files = glob.glob(os.path.join(self.datapath + 'val/LPCVC_Val/IMG/val', \"*.png\"))\n",
    "        return len(files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            img = cv2.imread(self.datapath + 'train/IMG/train/train_' + str(idx).zfill(4) + '.png')\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.imread(self.datapath + 'train/GT_Updated/train/train_' + str(idx).zfill(4) + '.png')\n",
    "        else:\n",
    "            img = cv2.imread(self.datapath + 'val/LPCVC_Val/GT/val/val_' + str(idx).zfill(4) + '.png')\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.imread(self.datapath + 'val/LPCVC_Val/IMG/val/val_' + str(idx).zfill(4) + '.png')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(0, 1)])\n",
    "        img = t(img)\n",
    "        mask = self.onehot(mask, self.n_class)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def onehot(self, img, nb):\n",
    "        oh = np.zeros((img.shape[0], img.shape[1], nb))\n",
    "        for i in range(nb):\n",
    "            oh[:, :, i] = (img[:, :, 0] == i)\n",
    "        return oh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "train_dataset = LPCVCDataset(\"dataset/\",transform=None,  n_class=14, train=True)\n",
    "val_dataset = LPCVCDataset(\"dataset/\", transform=None,  n_class=14, train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n",
      "(512, 512, 14)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].size()) # shape of input\n",
    "print(train_dataset[0][1].shape) # shape of output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resnet Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the device to use\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "created on 20/06/2023, building a resnet18 by scratch\n",
    "\"\"\"\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet_18(nn.Module):\n",
    "\n",
    "    def __init__(self, image_channels, num_classes):\n",
    "\n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "\n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "\n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride),\n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "resnet_model = ResNet_18(3, 14)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resnet on main branch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    # Scale factor of the number of output channels\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, is_first_block=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: number of input channels\n",
    "            out_channels: number of output channels\n",
    "            stride: stride using in (a) 3x3 convolution and\n",
    "                    (b) 1x1 convolution used for downsampling for skip connection\n",
    "            is_first_block: whether it is the first residual block of the layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Skip connection goes through 1x1 convolution with stride=2 for\n",
    "        # the first blocks of conv3_x, conv4_x, and conv5_x layers for matching\n",
    "        # spatial dimension of feature maps and number of channels in order to\n",
    "        # perform the add operations.\n",
    "        self.downsample = None\n",
    "        if is_first_block:\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels*self.expansion, kernel_size=1, stride=stride, padding=0), nn.BatchNorm2d(out_channels*self.expansion))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input\n",
    "        Returns:\n",
    "            Residual block output\n",
    "        \"\"\"\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    # Scale factor of the number of output channels\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, is_first_block=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: number of input channels\n",
    "            out_channels: number of output channels\n",
    "            stride: stride using in (a) the first 3x3 convolution and\n",
    "                    (b) 1x1 convolution used for downsampling for skip connection\n",
    "            is_first_block: whether it is the first residual block of the layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Skip connection goes through 1x1 convolution with stride=2 for\n",
    "        # the first blocks of conv3_x, conv4_x, and conv5_x layers for matching\n",
    "        # spatial dimension of feature maps and number of channels in order to\n",
    "        # perform the add operations.\n",
    "        self.downsample = None\n",
    "        if is_first_block and stride != 1:\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=0), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input\n",
    "        Returns:\n",
    "            Residual block ouput\n",
    "        \"\"\"\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, n_classes, n_blocks_list=[3, 4, 6, 3], out_channels_list=[64, 128, 256, 512], num_channels=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ResBlock: residual block type, BasicBlock for ResNet-18, 34 or\n",
    "                      BottleNeck for ResNet-50, 101, 152\n",
    "            n_class: number of classes for image classifcation (used in classfication head)\n",
    "            n_block_lists: number of residual blocks for each conv layer (conv2_x - conv5_x)\n",
    "            out_channels_list: list of the output channel numbers for conv2_x - conv5_x\n",
    "            num_channels: the number of channels of input image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=num_channels, out_channels=64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        # Create four convoluiontal layers\n",
    "        in_channels = 64\n",
    "        # For the first block of the second layer, do not downsample and use stride=1.\n",
    "        self.conv2_x = self.CreateLayer(ResBlock, n_blocks_list[0], in_channels, out_channels_list[0], stride=1)\n",
    "\n",
    "        # For the first blocks of conv3_x - conv5_x layers, perform downsampling using stride=2.\n",
    "        # By default, ResBlock.expansion = 4 for ResNet-50, 101, 152,\n",
    "        # ResBlock.expansion = 1 for ResNet-18, 34.\n",
    "        self.conv3_x = self.CreateLayer(ResBlock, n_blocks_list[1], out_channels_list[0]*ResBlock.expansion, out_channels_list[1], stride=2)\n",
    "        self.conv4_x = self.CreateLayer(ResBlock, n_blocks_list[2], out_channels_list[1]*ResBlock.expansion, out_channels_list[2], stride=2)\n",
    "        self.conv5_x = self.CreateLayer(ResBlock, n_blocks_list[3], out_channels_list[2]*ResBlock.expansion, out_channels_list[3], stride=2)\n",
    "\n",
    "        # Average pooling (used in classification head)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # MLP for classification (used in classification head)\n",
    "        self.fc = nn.Linear(out_channels_list[3] * ResBlock.expansion, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input image\n",
    "        Returns:\n",
    "            C2: feature maps after conv2_x\n",
    "            C3: feature maps after conv3_x\n",
    "            C4: feature maps after conv4_x\n",
    "            C5: feature maps after conv5_x\n",
    "            y: output class\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Feature maps\n",
    "        C2 = self.conv2_x(x)\n",
    "        C3 = self.conv3_x(C2)\n",
    "        C4 = self.conv4_x(C3)\n",
    "        C5 = self.conv5_x(C4)\n",
    "\n",
    "        # Classification head\n",
    "        y = self.avgpool(C5)\n",
    "        y = y.reshape(y.shape[0], -1)\n",
    "        y = self.fc(y)\n",
    "\n",
    "        return C2, C3, C4, C5, y\n",
    "\n",
    "\n",
    "    def CreateLayer(self, ResBlock, n_blocks, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Create a layer with specified type and number of residual blocks.\n",
    "        Args:\n",
    "            ResBlock: residual block type, BasicBlock for ResNet-18, 34 or\n",
    "                      BottleNeck for ResNet-50, 101, 152\n",
    "            n_blocks: number of residual blocks\n",
    "            in_channels: number of input channels\n",
    "            out_channels: number of output channels\n",
    "            stride: stride used in the first 3x3 convolution of the first resdiual block\n",
    "            of the layer and 1x1 convolution for skip connection in that block\n",
    "        Returns:\n",
    "            Convolutional layer\n",
    "        \"\"\"\n",
    "        layer = []\n",
    "        for i in range(n_blocks):\n",
    "            if i == 0:\n",
    "                # Downsample the feature map using input stride for the first block of the layer.\n",
    "                layer.append(ResBlock(in_channels, out_channels, stride=stride, is_first_block=True))\n",
    "            else:\n",
    "                # Keep the feature map size same for the rest three blocks of the layer.\n",
    "                # by setting stride=1 and is_first_block=False.\n",
    "                # By default, ResBlock.expansion = 4 for ResNet-50, 101, 152,\n",
    "                # ResBlock.expansion = 1 for ResNet-18, 34.\n",
    "                layer.append(ResBlock(out_channels*ResBlock.expansion, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "\n",
    "def GetFeatureMapsFromResnet(net, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        net: network input from torchvision.\n",
    "        x: input image\n",
    "    Returns:\n",
    "        C2: feature maps after conv2_x\n",
    "        C3: feature maps after conv3_x\n",
    "        C4: feature maps after conv4_x\n",
    "        C5: feature maps after conv5_x\n",
    "    \"\"\"\n",
    "    x = net.conv1(x)\n",
    "    x = net.bn1(x)\n",
    "    x = net.relu(x)\n",
    "    x = net.maxpool(x)\n",
    "    C2 = net.layer1(x)\n",
    "    C3 = net.layer2(C2)\n",
    "    C4 = net.layer3(C3)\n",
    "    C5 = net.layer4(C4)\n",
    "    return C2, C3, C4, C5\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "### Customed version ###\n",
    "# Resnet18\n",
    "net = ResNet(BasicBlock, 1000, n_blocks_list=[2, 2, 2, 2])\n",
    "# Resnet34\n",
    "#net = ResNet(BasicBlock, 1000)\n",
    "# Resnet50\n",
    "# net = ResNet(BottleNeck, 1000)\n",
    "# Resnet101\n",
    "#net = ResNet(BottleNeck, 1000, n_blocks_list=[3, 4, 23, 3])\n",
    "# Resnet152\n",
    "#net = ResNet(BottleNeck, 1000, n_blocks_list=[3, 8, 36, 3])\n",
    "x = torch.randn((1, 3, 512, 800), dtype=torch.float32)\n",
    "C2, C3, C4, C5, _ = net(x)\n",
    "\n",
    "### torchvision version ###\n",
    "net_tv = models.resnet18(pretrained=False)\n",
    "#net_tv = models.resnet34(pretrained=False)\n",
    "# net_tv = models.resnet50(pretrained=False)\n",
    "#net_tv = models.resnet101(pretrained=False)\n",
    "#net_tv = models.resnet152(pretrained=False)\n",
    "C2_tv, C3_tv, C4_tv, C5_tv = GetFeatureMapsFromResnet(net_tv, x)\n",
    "\n",
    "# print(\"Verifying the feature map shapes of customed ResNet and ResNet from torchvision\")\n",
    "# print(f\"C2.shape of customed ResNet: {C2.shape}\")\n",
    "# print(f\"C2.shape of torchvision ResNet: {C2_tv.shape}\")\n",
    "# print(f\"C3.shape of customed ResNet: {C3.shape}\")\n",
    "# print(f\"C3.shape of torchvision ResNet: {C3_tv.shape}\")\n",
    "# print(f\"C4.shape of customed ResNet: {C4.shape}\")\n",
    "# print(f\"C4.shape of torchvision ResNet: {C4_tv.shape}\")\n",
    "# print(f\"C5.shape of customed ResNet: {C5.shape}\")\n",
    "# print(f\"C5.shape of torchvision ResNet: {C5_tv.shape}\")\n",
    "#\n",
    "# print(\"Done!\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet_model = ResNet(BasicBlock, 14, n_blocks_list=[2, 2, 2, 2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move the model to the device\n",
    "resnet_model.to(device)\n",
    "next(resnet_model.parameters()).is_cuda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "import argparse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "data": {
      "text/plain": "Namespace(batch_size=32, datapath='dataset/', dev='cpu', epochs=100, lr=0.1, momentum_sgd=0.9, weight_decay=0.0001)"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Information Removal at the bottleneck in Deep Neural Networks')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.1)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0001)\n",
    "# parser.add_argument('--dev', default=\"cuda:0\")\n",
    "parser.add_argument('--dev', default=\"cpu\")\n",
    "parser.add_argument('--momentum-sgd', type=float, default=0.9, metavar='M',\n",
    "                    help='Momentum')\n",
    "parser.add_argument('--datapath', default='dataset/')\n",
    "args = parser.parse_args(\"\")\n",
    "args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type outputs:  <class 'tuple'>\n",
      "tuple len:  5\n",
      "torch.Size([32, 64, 128, 128])\n",
      "type outputs[1]:  <class 'torch.Tensor'>\n",
      "torch.Size([32, 128, 64, 64])\n",
      "type labels:  <class 'torch.Tensor'>\n",
      "torch.Size([32, 512, 512, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 512, 512, 14])) must be the same as input size (torch.Size([32, 128, 64, 64]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-177-19dd652ac45a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[1;31m# print(\"output size: \", outputs.size())\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[1;31m# print(\"labels size: \", labels.size())\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m     \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    705\u001B[0m                                                   \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    706\u001B[0m                                                   \u001B[0mpos_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 707\u001B[1;33m                                                   reduction=self.reduction)\n\u001B[0m\u001B[0;32m    708\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    709\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   2978\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2979\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2980\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Target size ({}) must be the same as input size ({})\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2981\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2982\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary_cross_entropy_with_logits\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction_enum\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([32, 512, 512, 14])) must be the same as input size (torch.Size([32, 128, 64, 64]))"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "iteration = 0\n",
    "args.device = torch.device(args.dev)\n",
    "if args.dev != \"cpu\":\n",
    "    torch.cuda.set_device(args.device)\n",
    "model = resnet_model.to(args.device)\n",
    "args.criterion = torch.nn.BCEWithLogitsLoss().to(args.device)\n",
    "args.optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum_sgd, weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    iteration+=1\n",
    "\n",
    "    inputs, labels = data[0].to(args.device), data[1].to(args.device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs=model(inputs)\n",
    "        print(\"type outputs: \", type(outputs))\n",
    "        print(\"tuple len: \", len(outputs))\n",
    "        print(outputs[0].size())\n",
    "        print(\"type outputs[1]: \", type(outputs[1]))\n",
    "        print(outputs[1].size())\n",
    "        print(\"type labels: \", type(labels))\n",
    "        print(labels.size())\n",
    "\n",
    "        # print(\"output size: \", outputs.size())\n",
    "        # print(\"labels size: \", labels.size())\n",
    "        loss = args.criterion(outputs[1],labels)\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### passed train and test function\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:888sicx0) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">dry-salad-5</strong> at: <a href='https://wandb.ai/lpcvc/LPCVC/runs/888sicx0' target=\"_blank\">https://wandb.ai/lpcvc/LPCVC/runs/888sicx0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230621_141949-888sicx0\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:888sicx0). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8a86c15fcc64662903d0bcfe9259a86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.4"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>E:\\E_Code\\segmentation_trial\\wandb\\run-20230621_142201-s2rd7ga7</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/lpcvc/LPCVC/runs/s2rd7ga7' target=\"_blank\">wild-waterfall-6</a></strong> to <a href='https://wandb.ai/lpcvc/LPCVC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/lpcvc/LPCVC' target=\"_blank\">https://wandb.ai/lpcvc/LPCVC</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/lpcvc/LPCVC/runs/s2rd7ga7' target=\"_blank\">https://wandb.ai/lpcvc/LPCVC/runs/s2rd7ga7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]C:\\Users\\26236\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  0%|          | 0/32 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 512, 512, 14])) must be the same as input size (torch.Size([32, 14]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-143-ca6783faa110>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\nEpoch : %d'\u001B[0m\u001B[1;33m%\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 151\u001B[1;33m     \u001B[0mtrain_acc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    152\u001B[0m     \u001B[0mtest_acc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m     wandb.log(\n",
      "\u001B[1;32m<ipython-input-143-ca6783faa110>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, args, train_loader)\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mamp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautocast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    705\u001B[0m                                                   \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    706\u001B[0m                                                   \u001B[0mpos_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 707\u001B[1;33m                                                   reduction=self.reduction)\n\u001B[0m\u001B[0;32m    708\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    709\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Unet-tuto\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   2978\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2979\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2980\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Target size ({}) must be the same as input size ({})\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2981\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2982\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary_cross_entropy_with_logits\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction_enum\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([32, 512, 512, 14])) must be the same as input size (torch.Size([32, 14]))"
     ]
    }
   ],
   "source": [
    "def train(model, args, train_loader):\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "    iteration=0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    for data in tqdm(train_loader):\n",
    "        iteration+=1\n",
    "\n",
    "        inputs, labels = data[0].to(args.device), data[1].to(args.device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs=model(inputs)\n",
    "            loss = args.criterion(outputs,labels)\n",
    "\n",
    "        args.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        args.optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "    train_loss=running_loss/len(train_loader)\n",
    "    # train_loss=running_loss/31  # 1021/32\n",
    "    accu=100.*correct/total\n",
    "\n",
    "\n",
    "    train_accu.append(accu)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
    "    return(accu, train_loss)\n",
    "\n",
    "def test(model, args, val_loader):\n",
    "    model.eval()\n",
    "    running_loss=0\n",
    "    iteration=0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_loader):\n",
    "            iteration+=1\n",
    "\n",
    "            inputs, labels = data[0].to(args.device), data[1].to(args.device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs=model(inputs)\n",
    "                loss = args.criterion(outputs,labels)\n",
    "\n",
    "            args.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            args.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "    test_loss=running_loss/len(val_loader)\n",
    "    # test_loss=running_loss/3  # 100/32\n",
    "    accu=100.*correct/total\n",
    "\n",
    "\n",
    "    eval_accu.append(accu)\n",
    "    eval_losses.append(test_loss)\n",
    "\n",
    "    print('Test Loss: %.3f | Accuracy: %.3f'%(test_loss,accu))\n",
    "    return(accu, test_loss)\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Information Removal at the bottleneck in Deep Neural Networks')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.1)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0001)\n",
    "# parser.add_argument('--dev', default=\"cuda:0\")\n",
    "parser.add_argument('--dev', default=\"cpu\")\n",
    "parser.add_argument('--momentum-sgd', type=float, default=0.9, metavar='M',\n",
    "                    help='Momentum')\n",
    "parser.add_argument('--datapath', default='dataset/')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = torch.device(args.dev)\n",
    "if args.dev != \"cpu\":\n",
    "    torch.cuda.set_device(args.device)\n",
    "\n",
    "model = resnet_model.to(args.device)\n",
    "\n",
    "train_dataset = LPCVCDataset(datapath=args.datapath,transform=None,  n_class=14, train=True)\n",
    "# train_dataset = drone_dataset_train\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        # num_workers=2,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    ")\n",
    "\n",
    "# train_loader = train_loader\n",
    "\n",
    "val_dataset = LPCVCDataset(datapath=args.datapath, transform=None,  n_class=14, train=False)\n",
    "# val_dataset = drone_dataset_test\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        # num_workers=1,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    ")\n",
    "\n",
    "# val_loader = val_loader\n",
    "\n",
    "args.criterion = torch.nn.BCEWithLogitsLoss().to(args.device)\n",
    "args.optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum_sgd, weight_decay=args.weight_decay)\n",
    "\n",
    "train_accu = []\n",
    "train_losses = []\n",
    "\n",
    "eval_accu = []\n",
    "eval_losses = []\n",
    "\n",
    "# wandb.init(project=\"LPCVC\", entity='lpcvc')\n",
    "wandb.init(project=\"LPCVC\")\n",
    "wandb.run.name = \"resnet18_train\"\n",
    "wandb.config.epochs = args.epochs\n",
    "wandb.config.batch_size = args.batch_size\n",
    "wandb.config.learning_rate = args.lr\n",
    "wandb.config.weight_decay = args.weight_decay\n",
    "wandb.config.momentum = args.momentum_sgd\n",
    "wandb.config.train_dataset = train_dataset\n",
    "wandb.config.test_dataset = val_dataset\n",
    "# wandb.config.train_targets = train_dataset.targets\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    print('\\nEpoch : %d'%epoch)\n",
    "    train_acc, train_loss = train(model, args, train_loader)\n",
    "    test_acc, test_loss = test(model, args, val_loader)\n",
    "    wandb.log(\n",
    "        {\"train_acc\": train_acc, \"train_loss\": train_loss,\n",
    "        \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "unet-tuto",
   "language": "python",
   "display_name": "Unet-tuto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
